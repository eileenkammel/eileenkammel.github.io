<!doctype html>
<html lang="en" itemscope itemtype="http://schema.org/Person">
<head>
  <meta charset="utf-8">
  <!-- Site Meta Data -->
  <title>Can LLMs Describe What They See?</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="">
  <meta name="author" content="Eileen Kammel">

  <link rel="shortcut icon" href="">

  <!-- schema.org -->
  <meta itemprop="name" content="Large Language Musings">
  <meta itemprop="image" content="">
  <meta itemprop="description" content="">

  <!-- Style Meta Data -->
  <link rel="stylesheet" href="/theme/css/milligram.css" type="text/css" />
  <link rel="stylesheet" href="/theme/css/custom.css" type="text/css" />

  <!-- Feed Meta Data -->

  <!-- Twitter Feed -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="">
  <meta name="twitter:image" content="">

<meta name="twitter:creator" content="">
<meta name="twitter:url" content="/drafts/can-llms-describe-what-they-see.html">
<meta name="twitter:title" content="Large Language Musings ~ Can LLMs Describe What They See?">
<meta name="twitter:description" content="How well do AI language models describe what they see? This study evaluates their ability to generate and understand referring expressions using a visual reference game.">

<!-- Facebook Meta Data -->
<meta property="og:title" content="Large Language Musings ~ Can LLMs Describe What They See?" />
<meta property="og:description" content="How well do AI language models describe what they see? This study evaluates their ability to generate and understand referring expressions using a visual reference game." />
<meta property="og:image" content="" />
</head>

<body>
    <div class="container">

    <!-- Navbar -->
      <div class="navbar">
        <ul>
            <div>
                <li>
                    <a href=""><h3>Large Language Musings</h3></a>
                </li>
                <li>
                </li>
            </div>
        </ul>
      </div>

  <!-- Sidebar -->
    <sidebar>
        <ul class="static-item">
                <li><a href="/">Home</a></li>

                <li><a href="/pages/about-me.html">About Me</a></li>
        </ul>        

        <ul>
        </ul>

            <h2><br/>Links</h2>
            <ul>
                    <li><a href="https://open.spotify.com/playlist/6hx11ZpXIunQDCxq6OrHs7?si=bcec27f22e084d15">Best Playlist</a></li>
            </ul> 
   
        <p> 
                <span>
                    <a href="https://www.linkedin.com/in/eileen-kammel/" target="_blank">
                        <img class="social-icons-m" src="/theme/images/icons/linkedin.png">
                    </a>
                </span>
                <span>
                    <a href="https://github.com/eileenkammel/" target="_blank">
                        <img class="social-icons-m" src="/theme/images/icons/github.png">
                    </a>
                </span>
        </p>
        <p>
        </p>
        <p>
        </p>
    </sidebar>
    
    <maincontent>
<h2>
    <a href="/drafts/can-llms-describe-what-they-see.html" rel="bookmark" title="Permalink to Can LLMs Describe What They See?">Can LLMs Describe What They See?</a>
</h2>
<h3>
    <a href="/drafts/can-llms-describe-what-they-see.html" rel="bookmark" title="Permalink to Evaluating Multimodal LLMs through a Picture-Guessing Game">Evaluating Multimodal LLMs through a Picture-Guessing Game</a>
</h3>
<div>
    <span>
        <a href="/author/eileen-kammel.html">Eileen Kammel</a> |
        Tue 29 April 2025 |
        <a href="/category/llm.html" rel="bookmark" title="Permalink to LLM">LLM</a> |
                <span><a href="/tag/llm.html">#llm </a></span>
                <span><a href="/tag/linguistics.html">#linguistics </a></span>
                <span><a href="/tag/evaluation.html">#evaluation </a></span>
    </span>
    </em>
    <hr>
</div>

<div>
    <mainarticle>
    <p>As large language models (LLMs) become increasingly capable of handling both language and visual inputs, a critical question emerges: <strong>Can they use language in a grounded, functional way to refer to things in their visual environment?</strong> This study addresses that question by testing LLMs’ ability to both understand and generate <strong>referring expressions</strong>—descriptions like <em>“the red chair”</em> or <em>“the ball on the left”</em> that identify specific objects among several alternatives.</p>
<p>Using a picture-guessing reference game, I evaluated various multimodal LLMs in both comprehension and production roles to better understand their strengths and limitations in this important linguistic task.</p>
<hr>
<h4>Experimental Design</h4>
<p>The evaluation was structured as a two-player reference game:</p>
<ul>
<li>The <strong>Explainer</strong> describes a target image.</li>
<li>The <strong>Guesser</strong> selects the correct image from a set of four based on the description.</li>
</ul>
<p>Two distinct tasks were tested:</p>
<ul>
<li><strong>Production mode</strong>: The model generates a referring expression for an image.</li>
<li><strong>Comprehension mode</strong>: The model is given a human-made expression and must select the correct image.</li>
</ul>
<h4>Image Datasets</h4>
<p>Two datasets of differing visual complexity were used:</p>
<p><strong>TUNA Corpus</strong><br>
Depicts familiar furniture items such as desks and chairs with attributes like type, color, and orientation.</p>
<p><em>Example:</em><br>
<img alt="TUNA Example" src="images/tuna_ex.png"><br>
First image described by attribute matrix: <code>&lt;desk, red, large, front&gt;</code></p>
<p><strong>3D Shapes Dataset</strong><br>
Consists of synthetic scenes with abstract shapes described by more attributes, including shape, color, floor color, and rotation.</p>
<p><em>Example:</em><br>
<img alt="3D Shapes Example" src="images/3ds_ex.png"><br>
First image described by attribute matrix: <code>&lt;ball, small, blue, red, orange, front&gt;</code></p>
<hr>
<h4>Key Findings</h4>
<h5>Comprehension is Stronger than Production</h5>
<p>LLMs generally performed better when interpreting human-generated descriptions than when asked to produce their own. Most models could successfully choose the correct image when provided with a well-formed referring expression. However, their ability to generate such expressions was limited, often resulting in verbose or ambiguous outputs.</p>
<h5>Visual Complexity Reduces Accuracy</h5>
<p>Models showed significantly lower performance on the 3D Shapes dataset compared to the simpler TUNA corpus. As visual scenes became more abstract or attribute-rich, LLMs were more likely to omit essential identifying details or include irrelevant ones.</p>
<h5>Commercial Models Outperform Open-Weight Models</h5>
<p>Commercial models such as GPT-4o, Claude 3.5, and Gemini demonstrated better overall accuracy and efficiency in both modes. Open-weight models (e.g., InternVL2, Idefics) often produced overly detailed or inconsistent outputs and struggled especially with more complex images.</p>
<hr>
<h4>Performance Metrics</h4>
<h5>Accuracy of Target Identification</h5>
<table>
<thead>
<tr>
<th>Model</th>
<th>Production Accuracy (TUNA)</th>
<th>Comprehension Accuracy (TUNA)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Claude 3.5</td>
<td>62%</td>
<td>100%</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>82%</td>
<td>99%</td>
</tr>
<tr>
<td>Gemini 2.0</td>
<td>79%</td>
<td>100%</td>
</tr>
<tr>
<td>InternVL2-8B (Open)</td>
<td>41%</td>
<td>95%</td>
</tr>
</tbody>
</table>
<p>Models identified targets more accurately in comprehension mode, confirming that <strong>generation</strong> remains the more difficult task.</p>
<h5>Surplus Information in Generated Descriptions</h5>
<p>Models often included extraneous details not required to uniquely identify the target. While humans averaged fewer than two surplus words, open-weight models added three to ten extra descriptive terms on average.</p>
<p><em>Example comparison:</em><br>
<img alt="Surplus Information" src="insert-path/surplus-info.png"></p>
<hr>
<h4>Interpretation and Implications</h4>
<p>Although multimodal LLMs can recognize visual content, their ability to refer to specific elements in a concise, contrastive manner remains limited. Generated descriptions frequently resemble <strong>image captions</strong> rather than true referring expressions, often including unnecessary information while omitting critical identifying features.</p>
<p>This limitation has implications for tasks involving grounded language use, such as:</p>
<ul>
<li>Human–robot interaction</li>
<li>Assistive technologies</li>
<li>Interactive agents in complex visual environments</li>
</ul>
<p>Accurate referring expressions are essential for effective communication in these contexts, and current LLMs—especially open-weight ones—still struggle to meet this standard.</p>
<hr>
<h4>Limitations and Future Directions</h4>
<p>This evaluation relied on strict criteria for referring expressions based on brevity algorithms. Future studies could incorporate more human-like flexibility in evaluation, allowing for redundant but informative descriptions.</p>
<p>Further investigation is also needed into:</p>
<ul>
<li>Multi-turn dialogue settings</li>
<li>Enhanced prompt engineering</li>
<li>Improved image-description alignment verification</li>
</ul>
<p>In particular, understanding how LLMs select which attributes to mention and ensuring that descriptions correspond to real, existing images remain open challenges.</p>
<hr>
<h4>Conclusion</h4>
<p>Multimodal LLMs show promising comprehension abilities when interpreting referring expressions, particularly on simpler datasets. However, generating clear, efficient, and unambiguous expressions remains a significant challenge—especially in visually complex settings. While commercial models show more consistent behavior, all models evaluated demonstrate room for improvement in using context to generate truly referential language.</p>
<p>For a deeper look into the experimental setup, results, and detailed analyses, you can access the full thesis:</p>
<p><strong><a href="insert-link-to-thesis.pdf">Read the full thesis (PDF)</a></strong></p>
    </mainarticle>
</div>
<hr>
        
<div>
        <i>If you found the article helpful, please share or cite the article, and spread the word:</i>
            <p style="margin-top: 2%;">
                <span><a target="_blank" rel="nofollow" onclick="javascript:window.open(this.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=400,width=700');return false;" title="Twitter" href="https://twitter.com/share?url=/drafts/can-llms-describe-what-they-see.html&text=Can LLMs Describe What They See?&via="><img class="social-icons-a" src="/theme/images/icons/twitter.png"></a></span>
                <span><a target="_blank" title="Facebook" rel="nofollow" onclick="javascript:window.open(this.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=500,width=700');return false;" href="https://www.facebook.com/sharer.php?u=/drafts/can-llms-describe-what-they-see.html&t=Can LLMs Describe What They See?"><img class="social-icons-a" src="/theme/images/icons/facebook.png"></a></span>

                <a  target="_blank" title="Linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=/drafts/can-llms-describe-what-they-see.html&title=Can LLMs Describe What They See?" rel="nofollow" onclick="javascript:window.open(this.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=450,width=650');return false;"><img class="social-icons-a" src="/theme/images/icons/linkedin.png"></a>
            </p>
</div>
<hr>
    <p><i>For any feedback or corrections, please write in to: </i><b> eileen@kammel.dev </b></p>
        
    </maincontent>

  <!-- Analytics -->

  </div>
</body>

</html>