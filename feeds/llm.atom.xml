<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Large Language Musings - LLM</title><link href="https://eileenkammel.github.io/" rel="alternate"></link><link href="https://eileenkammel.github.io/feeds/llm.atom.xml" rel="self"></link><id>https://eileenkammel.github.io/</id><updated>2025-04-29T00:00:00+02:00</updated><entry><title>Can LLMs Use Context To Describe What They See?</title><link href="https://eileenkammel.github.io/can-llms-use-context-to-describe-what-they-see.html" rel="alternate"></link><published>2025-04-29T00:00:00+02:00</published><updated>2025-04-29T00:00:00+02:00</updated><author><name>Eileen Kammel</name></author><id>tag:eileenkammel.github.io,2025-04-29:/can-llms-use-context-to-describe-what-they-see.html</id><summary type="html">&lt;p&gt;How well do AI language models describe what they see? This study evaluates their ability to generate and understand referring expressions using a visual reference game.&lt;/p&gt;</summary><content type="html">&lt;p&gt;As language models get better at understanding both text and images, I found myself wondering: &lt;strong&gt;Can they actually refer to something the way we do?&lt;/strong&gt; Not just describe an image, but &lt;em&gt;point something out&lt;/em&gt; within a controlled context clearly, like saying &lt;em&gt;“the red chair”&lt;/em&gt; when there are four different chairs.&lt;/p&gt;
&lt;p&gt;That’s what my thesis is all about. I set up a reference game, kind of like a digital version of "guess which picture I mean" and tested how well multi-modal LLMs could play it. The idea was to see whether they could understand and generate short, clear descriptions that help identify specific images while excluding the others. Here's what I found.&lt;/p&gt;
&lt;hr&gt;
&lt;h4&gt;The Setup: A Two-Player Game&lt;/h4&gt;
&lt;p&gt;The core of the experiment was a simple two-player game:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;strong&gt;Explainer&lt;/strong&gt; describes a target image.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;Guesser&lt;/strong&gt; picks the right one from a set of four.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We ran this in two modes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Production mode&lt;/strong&gt;: The model has to describe the image itself.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Comprehension mode&lt;/strong&gt;: The model is given a human-made description and has to guess the correct image.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;The Datasets&lt;/h5&gt;
&lt;p&gt;To make things interesting, I used two types of images, one simple, one complex:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;TUNA Corpus&lt;/strong&gt;&lt;br&gt;
This one shows everyday furniture like chairs and desks. The objects are simple and familiar. The corpus can be found &lt;a href="https://www.abdn.ac.uk/ncs/departments/computing-science/research/overview/projects/#faq3"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Example:&lt;/em&gt;&lt;br&gt;
&lt;img alt="TUNA Example" src="images/tuna_ex.png"&gt;&lt;br&gt;
The first image is described by the attribute matrix: &lt;code&gt;&amp;lt;desk, red, large, front&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3D Shapes Dataset&lt;/strong&gt;&lt;br&gt;
This dataset is trickier. It has three-dimensional shapes in rooms, with more features like separate colors for the shape, floor and wall, and rotation of the room. The corpus can be found &lt;a href="https://github.com/google-deepmind/3d-shapes"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Example:&lt;/em&gt;&lt;br&gt;
&lt;img alt="3D Shapes Example" src="images/3ds_ex.png"&gt;&lt;br&gt;
The first image is described by the attribute matrix: &lt;code&gt;&amp;lt;ball, small, blue, red wall, orange floor, front&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h4&gt;What I Learned&lt;/h4&gt;
&lt;h5&gt;Models Are Better at Understanding Than Describing&lt;/h5&gt;
&lt;p&gt;Most models did a pretty good job when they were just reading a human-written description and picking an image. But when they had to &lt;strong&gt;generate&lt;/strong&gt; that description themselves, things got messier. The descriptions were often too long or didn’t include the right details, so another LLM was not able to pick the right image based on such a description.&lt;/p&gt;
&lt;h5&gt;Harder Images Made Things Worse&lt;/h5&gt;
&lt;p&gt;Not surprisingly, the more complex 3D shapes caused more trouble. There were more things to describe and more ways to get it wrong.&lt;/p&gt;
&lt;h5&gt;Commercial Models Had the Upper Hand&lt;/h5&gt;
&lt;p&gt;Commercial models like GPT-4o and Claude 3.5 outperformed open models like InternVL2 or Idefics, especially when it came to generating clearer, more focused descriptions.&lt;/p&gt;
&lt;hr&gt;
&lt;h4&gt;Performance Overview&lt;/h4&gt;
&lt;p&gt;Here’s an overview of how models performed in different modes on both datasets:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Selection of correct image with LLM -ade expressions:&lt;/em&gt;&lt;br&gt;
&lt;img alt="Production Mode" src="images/completion_correct_ratio_all.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Selection of correct image with human-made expressions:&lt;/em&gt;&lt;br&gt;
&lt;img alt="Comprehension Mode" src="images/completion_correct_ratio_all_programmatic.png"&gt;&lt;/p&gt;
&lt;p&gt;In general, &lt;strong&gt;comprehension mode&lt;/strong&gt; (expressions made by humans) led to much higher accuracy than production mode. When given a good description, models were usually able to find the right image.&lt;/p&gt;
&lt;hr&gt;
&lt;h4&gt;A Closer Look at the Language&lt;/h4&gt;
&lt;h5&gt;Too Much Info&lt;/h5&gt;
&lt;p&gt;One common issue was that models added too much information. Instead of just saying &lt;em&gt;"the ball"&lt;/em&gt;, what would be sufficient within the given context, they’d say something like &lt;em&gt;"a small red rubber ball with a smooth surface placed in the middle of the orange-floored room with red walls."&lt;/em&gt; That might sound nice, but it’s not always helpful.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Surplus info comparison:&lt;/em&gt;&lt;br&gt;
&lt;img alt="Surplus Information" src="images/surplus_info_commercial.png"&gt;&lt;/p&gt;
&lt;p&gt;Human descriptions were much more efficient—short and to the point.&lt;/p&gt;
&lt;h5&gt;Visual Context &lt;em&gt;Does&lt;/em&gt; Indeed Influence Language&lt;/h5&gt;
&lt;p&gt;All models have additionally been prompted to give a &lt;strong&gt;ground truth&lt;/strong&gt; image description without the context of other images. Those descriptions exceed the picture guessing descriptions by far:&lt;/p&gt;
&lt;table border="1" cellpadding="8" cellspacing="0" style="border-collapse: collapse; text-align: left; width: 100%;"&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style="width: 25%;"&gt;Model&lt;/th&gt;
      &lt;th colspan="2" style="text-align: center;"&gt;Mean Diff (TUNA)&lt;/th&gt;
      &lt;th colspan="2" style="text-align: center;"&gt;Mean Diff (3DS)&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th style="text-align: center;"&gt;Info&lt;/th&gt;
      &lt;th style="text-align: center;"&gt;Length&lt;/th&gt;
      &lt;th style="text-align: center;"&gt;Info&lt;/th&gt;
      &lt;th style="text-align: center;"&gt;Length&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href="https://arxiv.org/abs/2410.21276" target="_blank"&gt;GPT-4o&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;14.17&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;34.81&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;18.2&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;40.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href="https://www.anthropic.com/news/claude-3-5-sonnet" target="_blank"&gt;Claude-3.5&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;31.66&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;74.03&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;30.55&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;73.25&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href="https://ai.google.dev/gemini-api/docs/models/experimental-models" target="_blank"&gt;Gemini-2.0&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;29.99&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;58.31&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;36.17&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;66.63&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href="https://arxiv.org/abs/2306.16527" target="_blank"&gt;Idefics&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;18.07&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;49.3&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;18.23&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;45.17&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href="https://arxiv.org/abs/2412.05271" target="_blank"&gt;InternVL2-Llama3-76B&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;30.22&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;67.89&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;26.45&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;61.96&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href="https://arxiv.org/abs/2412.05271" target="_blank"&gt;InternVL2-40B&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;29.52&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;72.7&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;30.57&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;70.33&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;a href="https://arxiv.org/abs/2412.05271" target="_blank"&gt;InternVL2-8B&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;26.33&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;60.26&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;24.29&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;58.54&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p style="margin-top: 8px; text-align: left; max-width: 800px;"&gt;
  &lt;strong&gt;Comparison of Mean Information Difference and Mean Length Difference in Ground Truth vs. Referring Expressions.&lt;/strong&gt;&lt;br&gt;
  (Information: Sum of included adjectives and nouns, Length: Word count) Positive values indicate more Information / word count in Ground Truth.
&lt;/p&gt;

&lt;hr&gt;
&lt;h4&gt;So, Can They Actually Do It?&lt;/h4&gt;
&lt;h5&gt;The Good News&lt;/h5&gt;
&lt;p&gt;Yes — &lt;strong&gt;LLMs can understand&lt;/strong&gt; referring expressions when they’re well written. Most models picked the right image when given a clear human-made description.&lt;/p&gt;
&lt;h5&gt;The Not-So-Good News&lt;/h5&gt;
&lt;p&gt;They’re still &lt;strong&gt;not great at writing those descriptions themselves&lt;/strong&gt;, especially in complex scenes. The descriptions often lacked the crucial bits, or included too much unrelated info. A lot of them ended up sounding more like image captions than like purposeful, contrastive expressions.&lt;/p&gt;
&lt;hr&gt;
&lt;h4&gt;Why This Matters&lt;/h4&gt;
&lt;p&gt;Being able to refer to things clearly is a big deal in real-world AI applications:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Helping robots understand and follow instructions&lt;/li&gt;
&lt;li&gt;Making assistive tools (like screen readers) smarter&lt;/li&gt;
&lt;li&gt;Improving collaboration between people and machines&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If a model says &lt;em&gt;“the chair in the back corner”&lt;/em&gt; but there are three of them, it’s a problem. That’s why it’s important we keep testing and improving this kind of ability.&lt;/p&gt;
&lt;hr&gt;
&lt;h4&gt;What Could Be Better?&lt;/h4&gt;
&lt;p&gt;Some ideas for future work:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Try more human-like evaluation methods (not just minimal expressions)&lt;/li&gt;
&lt;li&gt;Test how models behave in &lt;strong&gt;multi-turn&lt;/strong&gt; conversations&lt;/li&gt;
&lt;li&gt;See if models can be prompted or trained to use visual context more effectively&lt;/li&gt;
&lt;li&gt;Fix inconsistent outputs; models often gave different answers for the same input&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h4&gt;Final Thoughts&lt;/h4&gt;
&lt;p&gt;Language models are clearly getting better at handling vision and language together. But this kind of &lt;strong&gt;reference-focused task&lt;/strong&gt; is still tricky. Most models do fine reading descriptions, but struggle to produce their own that are short, clear, and helpful.&lt;/p&gt;
&lt;p&gt;Still, the gap is closing and this kind of research helps us understand where the real challenges are.&lt;/p&gt;
&lt;p&gt;Want to dig deeper?&lt;br&gt;
&lt;strong&gt;&lt;a href="https://drive.google.com/file/d/1cqocdQEf3c_h90fOwwvgsSTZ18eBanKJ/view?usp=sharing"&gt;Read the full thesis (PDF)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;</content><category term="LLM"></category><category term="llm"></category><category term="linguistics"></category><category term="evaluation"></category></entry></feed>